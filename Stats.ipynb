{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de4db0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1q Properties of F-distribution\\n1. Positively Skewed: The F-distribution is not symmetric and skews to the right.\\n\\n2. Two Degrees of Freedom: It depends on two degrees of freedom—one for the numerator and one for the denominator.\\n\\n3. Shape Changes with Degrees of Freedom: As the degrees of freedom increase, the distribution becomes less skewed.\\n\\n4. Non-Negative: F-values are always non-negative, as they represent a ratio of variances.\\n\\n5. Range: It only takes values \\\\( \\\\geq 0 \\\\); there are no negative values in the F-distribution.\\n\\n6. Mode: The distribution has a peak or mode, especially noticeable when degrees of freedom are low.\\n\\n7. Right-Tailed Tests: The F-distribution is commonly used for right-tailed tests in hypothesis testing.\\n\\n8. Variance Comparison: Ideal for comparing variances, as in ANOVA and regression significance testing.\\n\\n9. Related to Chi-Square: The F-distribution is derived from a ratio of two chi-square distributions.\\n\\n10. Used in ANOVA and Regression: Most often applied in ANOVA to test for variance differences among groups and in regression for model fit testing.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"1q Properties of F-distribution\n",
    "1. Positively Skewed: The F-distribution is not symmetric and skews to the right.\n",
    "\n",
    "2. Two Degrees of Freedom: It depends on two degrees of freedom—one for the numerator and one for the denominator.\n",
    "\n",
    "3. Shape Changes with Degrees of Freedom: As the degrees of freedom increase, the distribution becomes less skewed.\n",
    "\n",
    "4. Non-Negative: F-values are always non-negative, as they represent a ratio of variances.\n",
    "\n",
    "5. Range: It only takes values \\( \\geq 0 \\); there are no negative values in the F-distribution.\n",
    "\n",
    "6. Mode: The distribution has a peak or mode, especially noticeable when degrees of freedom are low.\n",
    "\n",
    "7. Right-Tailed Tests: The F-distribution is commonly used for right-tailed tests in hypothesis testing.\n",
    "\n",
    "8. Variance Comparison: Ideal for comparing variances, as in ANOVA and regression significance testing.\n",
    "\n",
    "9. Related to Chi-Square: The F-distribution is derived from a ratio of two chi-square distributions.\n",
    "\n",
    "10. Used in ANOVA and Regression: Most often applied in ANOVA to test for variance differences among groups and in regression for model fit testing.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabc1a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2q. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\\n\\n1. Analysis of Variance (ANOVA): ANOVA uses the F-distribution to test whether the variances between group means are significantly different, helping to identify if at least one group mean is different from others.\\n\\n2. Regression Analysis: In multiple regression, the F-distribution tests the overall significance of the model by comparing explained versus unexplained variance, assessing whether the model fits the data better than chance.\\n\\n3. Comparing Two Variances: The F-test is used to compare the variances of two independent samples, determining if they are statistically different, often as a preliminary test for equality of variance assumptions in other analyses.\\n\\n4. Nested Model Testing: The F-distribution is used to compare nested models, where a simpler model is a subset of a more complex one, to check if the additional parameters significantly improve the fit.\\n\\n5. MANOVA (Multivariate ANOVA): Like ANOVA, but extended to multiple dependent variables, MANOVA uses the F-distribution to determine if group variances across combinations of dependent variables are significantly different.\\n\\n6. Factorial ANOVA: In factorial designs, which involve multiple factors, the F-distribution helps test the significance of main and interaction effects among factors.\\n\\n7. Suitability: The F-distribution is appropriate because it models the ratio of variances, is non-negative, and accounts for skew, making it ideal for variance comparisons, especially in models with multiple parameters or groups.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''2q. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
    "\n",
    "1. Analysis of Variance (ANOVA): ANOVA uses the F-distribution to test whether the variances between group means are significantly different, helping to identify if at least one group mean is different from others.\n",
    "\n",
    "2. Regression Analysis: In multiple regression, the F-distribution tests the overall significance of the model by comparing explained versus unexplained variance, assessing whether the model fits the data better than chance.\n",
    "\n",
    "3. Comparing Two Variances: The F-test is used to compare the variances of two independent samples, determining if they are statistically different, often as a preliminary test for equality of variance assumptions in other analyses.\n",
    "\n",
    "4. Nested Model Testing: The F-distribution is used to compare nested models, where a simpler model is a subset of a more complex one, to check if the additional parameters significantly improve the fit.\n",
    "\n",
    "5. MANOVA (Multivariate ANOVA): Like ANOVA, but extended to multiple dependent variables, MANOVA uses the F-distribution to determine if group variances across combinations of dependent variables are significantly different.\n",
    "\n",
    "6. Factorial ANOVA: In factorial designs, which involve multiple factors, the F-distribution helps test the significance of main and interaction effects among factors.\n",
    "\n",
    "7. Suitability: The F-distribution is appropriate because it models the ratio of variances, is non-negative, and accounts for skew, making it ideal for variance comparisons, especially in models with multiple parameters or groups.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "952d5515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"3q. What are the key assumptions required for conducting an F-test to compare the variances of two\\npopulations?\\n\\n\\n1. Normality of Populations: Both populations should be approximately normally distributed. This assumption is crucial because the F-test is sensitive to deviations from normality, which can impact the test's accuracy.\\n\\n2. Independence of Observations: Observations within each sample and between the two samples must be independent. This means that one observation does not influence another.\\n\\n3. Random Sampling: Samples should be randomly selected from their respective populations to minimize bias and make the samples representative of the populations.\\n\\n4. Homogeneity of Variances: The F-test compares two variances, so it’s necessary to ensure that the two variances under consideration are meaningful in the context of similar data structures or groups.\\n\\n5. Sample Size Consideration: Generally, F-tests work best with reasonably large sample sizes. Small sample sizes might reduce the test's power or robustness, especially under non-normal conditions.\\n\\n6. No Outliers: Since F-tests are sensitive to extreme values, outliers can skew variances, leading to misleading results. Outliers should be examined and addressed prior to the test.\\n\\n7. One-Tailed or Two-Tailed Decision: Decide on the nature of the test (one-tailed or two-tailed) based on the hypothesis. The F-test can be tailored to detect if one variance is greater or if the variances are simply different.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''3q. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
    "populations?\n",
    "\n",
    "\n",
    "1. Normality of Populations: Both populations should be approximately normally distributed. This assumption is crucial because the F-test is sensitive to deviations from normality, which can impact the test's accuracy.\n",
    "\n",
    "2. Independence of Observations: Observations within each sample and between the two samples must be independent. This means that one observation does not influence another.\n",
    "\n",
    "3. Random Sampling: Samples should be randomly selected from their respective populations to minimize bias and make the samples representative of the populations.\n",
    "\n",
    "4. Homogeneity of Variances: The F-test compares two variances, so it’s necessary to ensure that the two variances under consideration are meaningful in the context of similar data structures or groups.\n",
    "\n",
    "5. Sample Size Consideration: Generally, F-tests work best with reasonably large sample sizes. Small sample sizes might reduce the test's power or robustness, especially under non-normal conditions.\n",
    "\n",
    "6. No Outliers: Since F-tests are sensitive to extreme values, outliers can skew variances, leading to misleading results. Outliers should be examined and addressed prior to the test.\n",
    "\n",
    "7. One-Tailed or Two-Tailed Decision: Decide on the nature of the test (one-tailed or two-tailed) based on the hypothesis. The F-test can be tailored to detect if one variance is greater or if the variances are simply different.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a72b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4. What is the purpose of ANOVA, and how does it differ from a t-test? \\nANOVA (Analysis of Variance) and the t-test both assess group differences, but they serve different purposes and are used in distinct scenarios:\\n\\n1. Purpose of ANOVA: ANOVA is designed to compare the means of three or more groups to determine if there is a statistically significant difference among them. It is particularly useful in experiments with multiple levels or factors. The goal is to assess if observed variations between group means are likely due to random chance or if at least one group mean is significantly different.\\n\\n2. Purpose of the t-test: The t-test compares the means of two groups to assess whether there is a significant difference between them. It is simpler than ANOVA and is suitable when only two group means are involved.\\n\\n3. Multiple Comparisons: Unlike the t-test, ANOVA can handle comparisons across more than two groups simultaneously, reducing the risk of Type I error (false positives) that occurs when conducting multiple t-tests on multiple groups.\\n\\n4. Types of Variance: ANOVA examines both between-group variance (differences among group means) and within-group variance (variability within each group). This allows ANOVA to account for variation within groups, whereas the t-test only examines the difference between two means without addressing within-group variability.\\n\\n5. Post-Hoc Analysis: If ANOVA finds a significant difference among groups, post-hoc tests (like Tukey’s HSD) can pinpoint which specific groups differ. A t-test, on the other hand, does not require post-hoc testing because it’s limited to two groups.\\n\\n6. Assumptions: Both ANOVA and the t-test assume normal distribution and homogeneity of variances, but ANOVA’s results are more reliable for multiple groups because it consolidates the comparison into a single test.\\n\\n7. Application: Use a t-test for comparing two groups. For three or more groups, ANOVA is more appropriate because it provides a more comprehensive view of group differences while controlling for error rates across multiple comparisons'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''4. What is the purpose of ANOVA, and how does it differ from a t-test? \n",
    "ANOVA (Analysis of Variance) and the t-test both assess group differences, but they serve different purposes and are used in distinct scenarios:\n",
    "\n",
    "1. Purpose of ANOVA: ANOVA is designed to compare the means of three or more groups to determine if there is a statistically significant difference among them. It is particularly useful in experiments with multiple levels or factors. The goal is to assess if observed variations between group means are likely due to random chance or if at least one group mean is significantly different.\n",
    "\n",
    "2. Purpose of the t-test: The t-test compares the means of two groups to assess whether there is a significant difference between them. It is simpler than ANOVA and is suitable when only two group means are involved.\n",
    "\n",
    "3. Multiple Comparisons: Unlike the t-test, ANOVA can handle comparisons across more than two groups simultaneously, reducing the risk of Type I error (false positives) that occurs when conducting multiple t-tests on multiple groups.\n",
    "\n",
    "4. Types of Variance: ANOVA examines both between-group variance (differences among group means) and within-group variance (variability within each group). This allows ANOVA to account for variation within groups, whereas the t-test only examines the difference between two means without addressing within-group variability.\n",
    "\n",
    "5. Post-Hoc Analysis: If ANOVA finds a significant difference among groups, post-hoc tests (like Tukey’s HSD) can pinpoint which specific groups differ. A t-test, on the other hand, does not require post-hoc testing because it’s limited to two groups.\n",
    "\n",
    "6. Assumptions: Both ANOVA and the t-test assume normal distribution and homogeneity of variances, but ANOVA’s results are more reliable for multiple groups because it consolidates the comparison into a single test.\n",
    "\n",
    "7. Application: Use a t-test for comparing two groups. For three or more groups, ANOVA is more appropriate because it provides a more comprehensive view of group differences while controlling for error rates across multiple comparisons'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bd374da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\\nthan two groups.\\nWhen comparing more than two groups, a one-way ANOVA is often preferable to using multiple t-tests for the following reasons:\\n\\n1. Reduces Type I Error: Conducting multiple t-tests increases the probability of a Type I error (false positive). A one-way ANOVA controls this error by assessing all group differences simultaneously in a single test.\\n\\n2. Efficiency: One-way ANOVA is more efficient because it requires fewer calculations than performing several t-tests, each with different groups. This makes the process faster and more streamlined.\\n\\n3. Comprehensive Analysis: ANOVA considers the overall variance within and between groups, providing a broader view of group differences, while t-tests are limited to only two-group comparisons.\\n\\n4. Post-Hoc Testing: With ANOVA, if significant differences are found, post-hoc tests can be used to identify specific group differences. This step avoids multiple comparisons at the initial stage, reducing complexity and error risk.\\n\\n5. Applicability to Multiple Comparisons: One-way ANOVA is explicitly designed to handle three or more groups, making it the appropriate choice for multi-group analysis, whereas multiple t-tests are limited to pairwise comparisons and lack a holistic approach to group differences.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
    "than two groups.\n",
    "When comparing more than two groups, a one-way ANOVA is often preferable to using multiple t-tests for the following reasons:\n",
    "\n",
    "1. Reduces Type I Error: Conducting multiple t-tests increases the probability of a Type I error (false positive). A one-way ANOVA controls this error by assessing all group differences simultaneously in a single test.\n",
    "\n",
    "2. Efficiency: One-way ANOVA is more efficient because it requires fewer calculations than performing several t-tests, each with different groups. This makes the process faster and more streamlined.\n",
    "\n",
    "3. Comprehensive Analysis: ANOVA considers the overall variance within and between groups, providing a broader view of group differences, while t-tests are limited to only two-group comparisons.\n",
    "\n",
    "4. Post-Hoc Testing: With ANOVA, if significant differences are found, post-hoc tests can be used to identify specific group differences. This step avoids multiple comparisons at the initial stage, reducing complexity and error risk.\n",
    "\n",
    "5. Applicability to Multiple Comparisons: One-way ANOVA is explicitly designed to handle three or more groups, making it the appropriate choice for multi-group analysis, whereas multiple t-tests are limited to pairwise comparisons and lack a holistic approach to group differences.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75f8ac25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\\nHow does this partitioning contribute to the calculation of the F-statistic?\\nIn ANOVA (Analysis of Variance), variance is partitioned into between-group variance and within-group variance, each serving a distinct purpose in understanding the overall variability in the data.\\n\\n 1. Between-Group Variance\\n- Definition: This component measures how much the group means differ from the overall mean of all groups combined. It indicates the extent to which the groups vary from each other.\\n- Importance: A high between-group variance suggests that the groups have different effects or treatments, implying that the factor being studied may significantly influence the outcomes.\\n\\n 2. Within-Group Variance\\n- Definition: This measures the variability within each group. It reflects how individual observations differ from their respective group mean.\\n- Importance: A low within-group variance indicates that individual observations are similar, enhancing the reliability of the group mean as a representation of that group.\\n\\n 3. Contribution to the F-Statistic\\n- The F-statistic is calculated by taking the ratio of the average between-group variance to the average within-group variance. This ratio helps assess whether the observed differences among group means are greater than would be expected by chance.\\n\\n 4. Interpretation of the F-Statistic\\n- A larger F-statistic suggests that there is more variability between groups than within groups, indicating that at least one group mean is likely different from the others. Conversely, a smaller F-statistic implies that any differences in group means might be due to random chance rather than a true effect.\\n\\n 5. Summary of Importance\\n- By partitioning variance into these two components, ANOVA allows researchers to determine the significance of the differences among groups effectively. This helps in understanding whether the treatments or categories being analyzed have meaningful effects on the dependent variable.\\n\\nThis systematic approach enhances the analysis of data by isolating the effects of treatments while controlling for random variability, making ANOVA a powerful tool for hypothesis testing in various fields of research.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
    "How does this partitioning contribute to the calculation of the F-statistic?\n",
    "In ANOVA (Analysis of Variance), variance is partitioned into between-group variance and within-group variance, each serving a distinct purpose in understanding the overall variability in the data.\n",
    "\n",
    " 1. Between-Group Variance\n",
    "- Definition: This component measures how much the group means differ from the overall mean of all groups combined. It indicates the extent to which the groups vary from each other.\n",
    "- Importance: A high between-group variance suggests that the groups have different effects or treatments, implying that the factor being studied may significantly influence the outcomes.\n",
    "\n",
    " 2. Within-Group Variance\n",
    "- Definition: This measures the variability within each group. It reflects how individual observations differ from their respective group mean.\n",
    "- Importance: A low within-group variance indicates that individual observations are similar, enhancing the reliability of the group mean as a representation of that group.\n",
    "\n",
    " 3. Contribution to the F-Statistic\n",
    "- The F-statistic is calculated by taking the ratio of the average between-group variance to the average within-group variance. This ratio helps assess whether the observed differences among group means are greater than would be expected by chance.\n",
    "\n",
    " 4. Interpretation of the F-Statistic\n",
    "- A larger F-statistic suggests that there is more variability between groups than within groups, indicating that at least one group mean is likely different from the others. Conversely, a smaller F-statistic implies that any differences in group means might be due to random chance rather than a true effect.\n",
    "\n",
    " 5. Summary of Importance\n",
    "- By partitioning variance into these two components, ANOVA allows researchers to determine the significance of the differences among groups effectively. This helps in understanding whether the treatments or categories being analyzed have meaningful effects on the dependent variable.\n",
    "\n",
    "This systematic approach enhances the analysis of data by isolating the effects of treatments while controlling for random variability, making ANOVA a powerful tool for hypothesis testing in various fields of research.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2648e39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\\ndifferences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\\nComparing the classical (frequentist) approach to ANOVA with the Bayesian approach highlights several key differences in handling uncertainty, parameter estimation, and hypothesis testing:\\n\\n1. Handling Uncertainty:\\n   - Frequentist Approach: Uncertainty is often expressed through p-values and confidence intervals, focusing on long-run frequencies of outcomes under repeated sampling. The results are typically interpreted in a binary manner (reject or fail to reject the null hypothesis).\\n   - Bayesian Approach: Uncertainty is quantified using probability distributions, allowing for a more nuanced understanding of the parameter estimates. This approach provides a direct probability of hypotheses being true, reflecting a degree of belief.\\n\\n2. Parameter Estimation:\\n   - Frequentist Approach: Parameters (like means and variances) are considered fixed but unknown quantities. Estimation is done using point estimates, such as sample means, and the focus is on properties of these estimates under repeated sampling.\\n   - Bayesian Approach: Parameters are treated as random variables with prior distributions representing beliefs before observing data. This allows for updating these beliefs with data, resulting in posterior distributions that incorporate both prior knowledge and new evidence.\\n\\n3. Hypothesis Testing:\\n   - Frequentist Approach: Hypothesis testing is typically done using null and alternative hypotheses, with a focus on rejecting or failing to reject the null based on the significance level. The emphasis is on controlling Type I and Type II errors.\\n   - Bayesian Approach: Hypothesis testing is based on the posterior probabilities of the hypotheses, allowing researchers to evaluate evidence in favor of one hypothesis over another. This can lead to different conclusions compared to the frequentist approach.\\n\\n4. Modeling Flexibility:\\n   - Frequentist Approach: Typically relies on specific models and assumptions about the data distribution. It can be less flexible when dealing with complex models or incorporating prior knowledge.\\n   - Bayesian Approach: Offers greater flexibility in modeling, allowing for the incorporation of prior information and complex hierarchical structures, making it more adaptable to various research contexts.\\n\\n5. Interpretation of Results:\\n   - Frequentist Approach: Results are interpreted based on the significance of effects, often leading to dichotomous conclusions that may not fully capture the complexity of the data.\\n   - Bayesian Approach: Results provide a more comprehensive view, allowing researchers to express results in terms of the probability of various hypotheses, enabling more informative decision-making and interpretation.\\n\\nThese differences highlight how frequentist and Bayesian approaches provide distinct frameworks for analyzing data and interpreting results in ANOVA, influencing how uncertainty and evidence are perceived in statistical analysis.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
    "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
    "Comparing the classical (frequentist) approach to ANOVA with the Bayesian approach highlights several key differences in handling uncertainty, parameter estimation, and hypothesis testing:\n",
    "\n",
    "1. Handling Uncertainty:\n",
    "   - Frequentist Approach: Uncertainty is often expressed through p-values and confidence intervals, focusing on long-run frequencies of outcomes under repeated sampling. The results are typically interpreted in a binary manner (reject or fail to reject the null hypothesis).\n",
    "   - Bayesian Approach: Uncertainty is quantified using probability distributions, allowing for a more nuanced understanding of the parameter estimates. This approach provides a direct probability of hypotheses being true, reflecting a degree of belief.\n",
    "\n",
    "2. Parameter Estimation:\n",
    "   - Frequentist Approach: Parameters (like means and variances) are considered fixed but unknown quantities. Estimation is done using point estimates, such as sample means, and the focus is on properties of these estimates under repeated sampling.\n",
    "   - Bayesian Approach: Parameters are treated as random variables with prior distributions representing beliefs before observing data. This allows for updating these beliefs with data, resulting in posterior distributions that incorporate both prior knowledge and new evidence.\n",
    "\n",
    "3. Hypothesis Testing:\n",
    "   - Frequentist Approach: Hypothesis testing is typically done using null and alternative hypotheses, with a focus on rejecting or failing to reject the null based on the significance level. The emphasis is on controlling Type I and Type II errors.\n",
    "   - Bayesian Approach: Hypothesis testing is based on the posterior probabilities of the hypotheses, allowing researchers to evaluate evidence in favor of one hypothesis over another. This can lead to different conclusions compared to the frequentist approach.\n",
    "\n",
    "4. Modeling Flexibility:\n",
    "   - Frequentist Approach: Typically relies on specific models and assumptions about the data distribution. It can be less flexible when dealing with complex models or incorporating prior knowledge.\n",
    "   - Bayesian Approach: Offers greater flexibility in modeling, allowing for the incorporation of prior information and complex hierarchical structures, making it more adaptable to various research contexts.\n",
    "\n",
    "5. Interpretation of Results:\n",
    "   - Frequentist Approach: Results are interpreted based on the significance of effects, often leading to dichotomous conclusions that may not fully capture the complexity of the data.\n",
    "   - Bayesian Approach: Results provide a more comprehensive view, allowing researchers to express results in terms of the probability of various hypotheses, enabling more informative decision-making and interpretation.\n",
    "\n",
    "These differences highlight how frequentist and Bayesian approaches provide distinct frameworks for analyzing data and interpreting results in ANOVA, influencing how uncertainty and evidence are perceived in statistical analysis.'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "716bc5ab",
   "metadata": {},
   "source": [
    "8. Question: You have two sets of data representing the incomes of two different professions1\n",
    "V Profession A: [48, 52, 55, 60, 62'\n",
    "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
    "incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e284be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.089171974522293, 0.49304859900533904)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "# Data\n",
    "profession_a = [48, 52, 55, 60, 62]\n",
    "profession_b = [45, 50, 55, 52, 47]\n",
    "\n",
    "# Calculate variances\n",
    "var_a = np.var(profession_a, ddof=1)  # ddof=1 for sample variance\n",
    "var_b = np.var(profession_b, ddof=1)\n",
    "\n",
    "# Calculate F-statistic\n",
    "f_statistic = var_a / var_b if var_a > var_b else var_b / var_a  # Ensure F > 1 by dividing larger variance by smaller\n",
    "dof_a = len(profession_a) - 1\n",
    "dof_b = len(profession_b) - 1\n",
    "\n",
    "# Calculate p-value\n",
    "p_value = 2 * min(f.cdf(f_statistic, dof_a, dof_b), 1 - f.cdf(f_statistic, dof_a, dof_b))  # Two-tailed test\n",
    "\n",
    "f_statistic, p_value\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4200739e",
   "metadata": {},
   "source": [
    "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
    "average heights between three different regions with the following data1\n",
    "V Region A: [160, 162, 165, 158, 164'\n",
    "V Region B: [172, 175, 170, 168, 174'\n",
    "V Region C: [180, 182, 179, 185, 183'\n",
    "V Task: Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589f500a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67.87330316742101, 2.870664187937026e-07)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Data\n",
    "region_a = [160, 162, 165, 158, 164]\n",
    "region_b = [172, 175, 170, 168, 174]\n",
    "region_c = [180, 182, 179, 185, 183]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(region_a, region_b, region_c)\n",
    "\n",
    "f_statistic, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df3b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
